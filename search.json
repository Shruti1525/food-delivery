  [

    {
      "title": "Introduction",
      "tags": "",
      "keywords": "sample homepage",
      "url": "index.html",
      "summary": "",
      "htmlContent": "<h2 id='what-is-prakash-'>What is prakash ?</h2><p>Prakash is a robust and advanced Data Observability Platform designed to address the challenges of managing data stacks effectively.</p><p>It provides enterprises with a holistic solution for gaining visibility into their data and proactively identifying and addressing data issues.</p><p>With Prakash, enterprises can monitor, analyze, and govern their data stacks in real-time.</p><p>Prakash empowers enterprises to make data-driven decisions, improve operational efficiency, and mitigate risks.</p><h2 id='why-we-are-using-data-observabilitychallenges'>Why we are using data observability/challenges?</h2><ul><li>Enterprises face challenges in managing their data stacks efficiently and effectively</li><li>Gaining visibility into data issues and anomalies is difficult and often reactive rather than proactive.</li><li>Lack of comprehensive data profiling leads to limited understanding of data quality and potential problems.</li><li>Manual data quality checks are time-consuming, prone to errors, and hinder scalability.</li><li>Tracking data lineage across complex systems is a complex and challenging task</li><li>Inadequate data governance processes result in compliance and regulatory risks</li></ul><h2 id='how-do-i-started'>How do I start?</h2><p>Greetings, fellow explorer! If you’re eager to dive in, create an account and start now. If you prefer to learn first, here’s where to begin:</p><p><strong>Quick Start:</strong> Discover how to add observability to your data stack in under 10 minutes. Prepare for a speedy revelation!</p><p><strong>Seamless Integration:</strong> Link data warehouses, transformation tools, and business intelligence with a simple warehouse selection.</p><p><strong>Latest Insights:</strong> Immerse in our blog’s wisdom, where experts unravel the secrets of data observability.</p><p>Your journey to data observability begins now. Choose your path and set sail!</p><h2 id='benefits'>Benefits</h2><ul><li><p>Real-time visibility into data stacks</p></li><li><p>Proactive identification of data problems and anomalies</p></li><li><p>Enhanced decision-making based on comprehensive insights</p></li><li><p>Timely resolution of data issues to avoid disruptions</p></li><li><p>Automated data quality checks and validations</p></li><li><p>Identification and resolution of data anomalies and inconsistencies</p></li><li><p>Reliable and accurate data for improved analysis and reporting</p></li><li><p>Increased confidence in data-driven decision-making</p></li><li><p>Streamlined Governance and Compliance</p></li></ul>",
      "content" : "What is prakash ? Prakash is a robust and advanced Data Observability Platform designed to address the challenges of managing data stacks effectively. It provides enterprises with a holistic solution for gaining visibility into their data and proactively identifying and addressing data issues. With Prakash, enterprises can monitor, analyze, and govern their data stacks in real-time. Prakash empowers enterprises to make data-driven decisions, improve operational efficiency, and mitigate risks. Why we are using data observability/challenges? Enterprises face challenges in managing their data stacks efficiently and effectively Gaining visibility into data issues and anomalies is difficult and often reactive rather than proactive. Lack of comprehensive data profiling leads to limited understanding of data quality and potential problems. Manual data quality checks are time-consuming, prone to errors, and hinder scalability. Tracking data lineage across complex systems is a complex and challenging task Inadequate data governance processes result in compliance and regulatory risks How do I start? Greetings, fellow explorer! If you’re eager to dive in, create an account and start now. If you prefer to learn first, here’s where to begin: Quick Start: Discover how to add observability to your data stack in under 10 minutes. Prepare for a speedy revelation! Seamless Integration: Link data warehouses, transformation tools, and business intelligence with a simple warehouse selection. Latest Insights: Immerse in our blog’s wisdom, where experts unravel the secrets of data observability. Your journey to data observability begins now. Choose your path and set sail! Benefits Real-time visibility into data stacks Proactive identification of data problems and anomalies Enhanced decision-making based on comprehensive insights Timely resolution of data issues to avoid disruptions Automated data quality checks and validations Identification and resolution of data anomalies and inconsistencies Reliable and accurate data for improved analysis and reporting Increased confidence in data-driven decision-making Streamlined Governance and Compliance"
    }
    
  ,

  {
    "title": "Airflow",
    "tags": "",
    "keywords": "PDF, prince, prince XML, ant, xsl fo",
    "url": "mydoc_airflow.html",
    "summary": "",
    "htmlContent": "<p>In this section, we provide guides and references to use the Airflow connector.</p><p><strong>Step 1 –: Create New Service</strong></p><ul><li>Create New Service to click on + ADD.</li><li>The first step is to ingest the metadata from your sources. To do that, you first need to create a Service connection first.</li><li>This Service will be the bridge between Prakash and your source system<br /><img src='/images/airflow/airflow_one.png' alt='image' /></li></ul><p>The Add New service form should look something like this. <br /><img src='/images/airflow/airflow_two.png' alt='image' /></p><p><strong>Step 2 –: Select Airflow Pipeline Service Type</strong><br /><img src='/images/airflow/airflow_three.png' alt='image' /></p><p><strong>Step 3 –: Name and Describe Your Service</strong></p><p>Provide a name and description for your Service.</p><p><strong>Service Name:-</strong></p><ul><li><strong>Prakash</strong> uniquely identifies Services by their Service Name. Provide a name that distinguishes your deployment from other Services, including the other Airflow Services that you might be ingesting metadata from.</li></ul><p><strong>Note that when the name is set, it cannot be changed.</strong> <br /><img src='/images/airflow/airflow_four.png' alt='image' /></p><p><strong>Step 4 –: Configure The Service Connection</strong></p><ul><li>In this step, we will configure the connection settings required for Glue</li><li>Please follow the instructions below to properly configure the Service to read from your sources. You will also find helper documentation on the right-hand side panel in the UI <br /><img src='/images/airflow/airflow_five.png' alt='image' /></li></ul><h2 id='connection-details--'>Connection Details: -</h2><ul><li><strong>Host and Port:</strong> Host and port of the Airflow service. This should be specified as a string in the format hostname:port. E.g., adb-xyz.azuredatabricks.net:443</li><li><strong>Number Of Status:</strong> Number of past task status to read every time the ingestion runs. By default, we will pick up and update the last 10 runs.</li><li><strong>Metadata Database Connection:</strong> Select your underlying database connection.</li></ul><p>Note that the <strong>Backend Connection</strong> is only used to extract metadata from a DAG running directly in your instance.</p><p><strong>Step 5 –: Check Test Connection</strong></p><p>Once the credentials have been added, click on TEST CONNECTION To Check Credentials is valid or not.<br /><img src='/images/airflow/airflow_six.png' alt='image' /></p><p><strong>If Test Connection Successful after that click on SAVE and then configure Metadata Ingestion.</strong></p><p><strong>Step 6 –: Configure Metadata Ingestion</strong></p><p>In this step we will configure the metadata ingestion pipeline, please follow the instructions below.<br /><img src='/images/airflow/airflow_seven.png' alt='image' /></p><ul><li><strong>Pipeline Filter Pattern:</strong> Note that all of them support regex as include or exclude.</li><li><strong>Database Service Name:</strong> You can enter a list of Database Services that are hosting the inlet and the outlet tables.</li><li><strong>Include Tags:</strong> Set the ‘Include Tags’ toggle to control whether to include tags in metadata ingestion.</li><li><strong>Mark Deleted Pipeline:</strong> Set the ‘Mark Deleted Dashboards’ toggle to flag Pipeline as soft-deleted if they are not present anymore in the source system.</li><li><strong>Include lineage:</strong> Set the ‘Include Tags’ toggle to control whether to include tags as part of metadata ingestion.</li><li><strong>Enable Debug log:</strong> Set the Enable Debug Log toggle to set the default log level to debug.</li></ul><p><strong>Step 7 –: Schedule the Ingestion and Deploy</strong></p><ul><li>Scheduling can be set up at an hourly, daily, weekly, or manual cadence. The timezone is in UTC. Select a Start Date to schedule for ingestion. It is optional to add an End Date</li><li>Review your configuration settings. If they match what you intended, click DEPLOY to create the service and schedule metadata ingestion</li><li>If something doesn’t look right, click the BACK button to return to the appropriate step and change the settings as needed.</li><li>After configuring the workflow, you can click on DEPLOY to create the pipeline.  <br /><img src='/images/airflow/airflow_eight.png' alt='image' /></li></ul><p><strong>Step 8 –: View the Ingestion Pipeline</strong></p><p>Once the workflow has been successfully deployed, you can view the Ingestion Pipeline running from the Service Page<br /> <img src='/images/airflow/airflow_step_8.png' alt='image' /></p>",
    "content":"In this section, we provide guides and references to use the Airflow connector. Step 1 –: Create New Service Create New Service to click on + ADD. The first step is to ingest the metadata from your sources. To do that, you first need to create a Service connection first. This Service will be the bridge between Prakash and your source system image The Add New service form should look something like this. image Step 2 –: Select Airflow Pipeline Service Type image Step 3 –: Name and Describe Your Service Provide a name and description for your Service. Service Name:- Prakash uniquely identifies Services by their Service Name. Provide a name that distinguishes your deployment from other Services, including the other Airflow Services that you might be ingesting metadata from. Note that when the name is set, it cannot be changed. image Step 4 –: Configure The Service Connection In this step, we will configure the connection settings required for Glue Please follow the instructions below to properly configure the Service to read from your sources. You will also find helper documentation on the right-hand side panel in the UI image Connection Details: - Host and Port: Host and port of the Airflow service. This should be specified as a string in the format hostname:port. E.g., adb-xyz.azuredatabricks.net:443 Number Of Status: Number of past task status to read every time the ingestion runs. By default, we will pick up and update the last 10 runs. Metadata Database Connection: Select your underlying database connection. Note that the Backend Connection is only used to extract metadata from a DAG running directly in your instance. Step 5 –: Check Test Connection Once the credentials have been added, click on TEST CONNECTION To Check Credentials is valid or not. image If Test Connection Successful after that click on SAVE and then configure Metadata Ingestion. Step 6 –: Configure Metadata Ingestion In this step we will configure the metadata ingestion pipeline, please follow the instructions below. image Pipeline Filter Pattern: Note that all of them support regex as include or exclude. Database Service Name: You can enter a list of Database Services that are hosting the inlet and the outlet tables. Include Tags: Set the ‘Include Tags’ toggle to control whether to include tags in metadata ingestion. Mark Deleted Pipeline: Set the ‘Mark Deleted Dashboards’ toggle to flag Pipeline as soft-deleted if they are not present anymore in the source system. Include lineage: Set the ‘Include Tags’ toggle to control whether to include tags as part of metadata ingestion. Enable Debug log: Set the Enable Debug Log toggle to set the default log level to debug. Step 7 –: Schedule the Ingestion and Deploy Scheduling can be set up at an hourly, daily, weekly, or manual cadence. The timezone is in UTC. Select a Start Date to schedule for ingestion. It is optional to add an End Date Review your configuration settings. If they match what you intended, click DEPLOY to create the service and schedule metadata ingestion If something doesn’t look right, click the BACK button to return to the appropriate step and change the settings as needed. After configuring the workflow, you can click on DEPLOY to create the pipeline. image Step 8 –: View the Ingestion Pipeline Once the workflow has been successfully deployed, you can view the Ingestion Pipeline running from the Service Page image"
  }
  
  ,

  {
    "title": "App Analytics",
    "tags": "",
    "keywords": "questions, troubleshooting, contact, support",
    "url": "mydoc_app_analytics.html",
    "summary": "",
    "htmlContent": "<p>This report displays the organizational health at a glance with details on the Page Views by Data Assets, Daily Active Users on the Platform, and the Most Active User.<br /><img src='/images/app_analytics/app_anly_one.png' alt='image' /></p><h2 id='most-viewed-data-assets'>Most Viewed Data Assets</h2><p><img src='/images/app_analytics/app_anly_two.png' alt='image' /></p><h2 id='page-views-by-data-assets'>Page Views by Data Assets</h2><p>It helps to understand the total number of page views by asset type. This allows you to understand which asset family drives the most interest in your organization  <br /><img src='/images/app_analytics/app_anly_three.png' alt='image' /></p><h2 id='daily-active-users-on-the-platform'>Daily Active Users on the Platform</h2><p>Active users are users with at least one session. This report allows understanding the platform usage. <br /><img src='/images/app_analytics/app_anly_four.png' alt='image' /></p><h2 id='most-active-users'>Most Active Users</h2><p>This report displays the most active users on the platform based on Page Views. They are the power users in your data team<br /><img src='/images/app_analytics/app_anly_five.png' alt='image' /></p>",
    "content":"This report displays the organizational health at a glance with details on the Page Views by Data Assets, Daily Active Users on the Platform, and the Most Active User. image Most Viewed Data Assets image Page Views by Data Assets It helps to understand the total number of page views by asset type. This allows you to understand which asset family drives the most interest in your organization image Daily Active Users on the Platform Active users are users with at least one session. This report allows understanding the platform usage. image Most Active Users This report displays the most active users on the platform based on Page Views. They are the power users in your data team image"
  }
  
  ,

  {
    "title": "Athena",
    "tags": "",
    "keywords": "popovers, tooltips, user interface text, glossaries, definitions",
    "url": "mydoc_athena.html",
    "summary": "",
    "htmlContent": "<p>Guides and references for using the Athena connector.</p><p><strong>Step 1: Create New Service</strong></p><ul><li>Create a new service by clicking on + ADD.</li><li>Ingest metadata from sources by creating a Service connection.</li><li>This Service acts as a bridge between Prakash and your source system.<br /><img src='/images/athena/athena_one.png' alt='image' /></li><li>After creating a service, the form should resemble this.<br /><img src='/images/athena/athena_two.png' alt='image' /></li></ul><p><strong>Step 2: Select Athena Service Type</strong></p><ul><li>Choose Athena as the Service type and click NEXT.<br /><img src='/images/athena/athena_three.png' alt='image' /></li></ul><p><strong>Step 3: Name and Describe Your Service</strong></p><ul><li>Provide a unique name and description.</li></ul><p>Service Name<ul><li>Identifies services. Cannot be changed later.<br /><img src='/images/athena/athena_four.png' alt='image' /></li></ul></p><p><strong>Step 4: Configure the Service Connection</strong></p><ul><li>Configure connection settings for Athena.<br /><img src='/images/athena/athena_five.png' alt='image' /></li></ul><h2 id='connection-details'>Connection Details</h2><ul><li>AWS Access Key ID &amp; Secret Access Key: Credentials for AWS interaction.</li><li>AWS Region: Specify the region of the service.</li><li>AWS Session Token (optional): Needed for temporary credentials.</li><li>Endpoint URL (optional): Specify an alternate endpoint.</li><li>Profile Name: Use a profile other than default if needed.</li><li>Assume Role Arn: Required for AssumeRole.</li><li>Assume Role Session Name: Identifier for the assumed role session.</li><li>Assume Role Source Identity: Source identity for AssumeRole.</li><li>S3 Staging Directory (optional): Optional S3 staging directory.</li><li>Athena Workgroup (optional): Workgroup name for Athena connection.</li></ul><p><strong>Advanced Configuration</strong></p><ul><li>Connection Options (Optional): Additional connection options as Key-Value pairs.</li><li>Connection Arguments (Optional): Additional connection arguments as Key-Value pairs.<br /><img src='/images/athena/athena_six.png' alt='image' /></li></ul><p><strong>Step 5: Check Test Connection</strong></p><p>After adding credentials, click TEST CONNECTION to validate. If successful, click SAVE and configure Metadata Ingestion.<br /><img src='/images/athena/athena_seven.png' alt='image' /><br /><img src='/images/athena/athena_eight.png' alt='image' /></p><p><strong>Step 6: Configure Metadata Ingestion</strong></p><p>Configure the metadata ingestion pipeline.<br /><img src='/images/athena/athena_ten.png' alt='image' /></p><h2 id='metadata-ingestion-options'>Metadata Ingestion Options</h2><ul><li><strong>Name:</strong> Name of the ingestion pipeline.</li><li><strong>Database Filter Pattern (Optional):</strong> Control inclusion/exclusion of databases.</li><li><strong>Schema Filter Pattern (Optional):</strong> Control inclusion/exclusion of schemas.</li><li><strong>Table Filter Pattern (Optional):</strong> Control inclusion/exclusion of tables.<br /><img src='/images/athena/athena_eleven.png' alt='image' /></li></ul><ul><li><strong>Include views (toggle):</strong> Toggle to include views in metadata.</li><li><strong>Include tags (toggle):</strong> Toggle to include tags in metadata.</li><li><strong>Enable Debug Log (toggle):</strong> Toggle to set the default log level to debug.</li></ul>",
    "content": "Guides and references for using the Athena connector. Step 1: Create New Service Create a new service by clicking on + ADD. Ingest metadata from sources by creating a Service connection. This Service acts as a bridge between Prakash and your source system. image After creating a service, the form should resemble this. image Step 2: Select Athena Service Type Choose Athena as the Service type and click NEXT. image Step 3: Name and Describe Your Service Provide a unique name and description. Service Name Identifies services. Cannot be changed later. image Step 4: Configure the Service Connection Configure connection settings for Athena. image Connection Details AWS Access Key ID & Secret Access Key: Credentials for AWS interaction. AWS Region: Specify the region of the service. AWS Session Token (optional): Needed for temporary credentials. Endpoint URL (optional): Specify an alternate endpoint. Profile Name: Use a profile other than default if needed. Assume Role Arn: Required for AssumeRole. Assume Role Session Name: Identifier for the assumed role session. Assume Role Source Identity: Source identity for AssumeRole. S3 Staging Directory (optional): Optional S3 staging directory. Athena Workgroup (optional): Workgroup name for Athena connection. Advanced Configuration Connection Options (Optional): Additional connection options as Key-Value pairs. Connection Arguments (Optional): Additional connection arguments as Key-Value pairs. image Step 5: Check Test Connection After adding credentials, click TEST CONNECTION to validate. If successful, click SAVE and configure Metadata Ingestion. image image Step 6: Configure Metadata Ingestion Configure the metadata ingestion pipeline. image Metadata Ingestion Options Name: Name of the ingestion pipeline. Database Filter Pattern (Optional): Control inclusion/exclusion of databases. Schema Filter Pattern (Optional): Control inclusion/exclusion of schemas. Table Filter Pattern (Optional): Control inclusion/exclusion of tables. image Include views (toggle): Toggle to include views in metadata. Include tags (toggle): Toggle to include tags in metadata. Enable Debug Log (toggle): Toggle to set the default log level to debug."  }
  
  ]
